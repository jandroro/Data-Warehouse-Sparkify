# Sparkify - Data Modeling and ETL Pipeline

The main objective of the project is to model a database schema in Redshift and create an ETL pipeline using Python for a startup called Sparkify, which wants to move their processes and data onto the cloud to analyze and query the data about songs and user activity (logs) from its new music streaming app using cloud services.

## Analytical objectives

Build an ETL pipeline that extracts the Sparkify's data from S3, stages them in Redshift, and transforms data into a set of dimensional tables for their analytics team to continue finding insights in what songs their users are listening to.

## Project Datasets

### Song Dataset
The first dataset is a subset of real data from the [Million Song Dataset](https://labrosa.ee.columbia.edu/millionsong/). Each file is in JSON format and contains metadata about a song and the artist of that song. The files are partitioned by the first three letters of each song's track ID. For example, here are filepaths to two files in this dataset.

```
song_data/A/B/C/TRABCEI128F424C983.json
song_data/A/A/B/TRAABJL12903CDCF1A.json
```

And below is an example of what a single song file, TRAABJL12903CDCF1A.json, looks like.

```
{"num_songs": 1, "artist_id": "ARJIE2Y1187B994AB7", "artist_latitude": null, "artist_longitude": null, "artist_location": "", "artist_name": "Line Renaud", "song_id": "SOUPIRU12A6D4FA1E1", "title": "Der Kleine Dompfaff", "duration": 152.92036, "year": 0}
```

### Log Dataset
The second dataset consists of log files in JSON format generated by this event simulator based on the songs in the dataset above. These simulate activity logs from a music streaming app based on specified configurations.

The log files in the dataset you'll be working with are partitioned by year and month. For example, here are filepaths to two files in this dataset.

```
log_data/2018/11/2018-11-12-events.json
log_data/2018/11/2018-11-13-events.json
```

And below is an example of what the data in a log file, 2018-11-12-events.json, looks like.

![Log example](/assets/log-data.png)

## Database Schema

The project data model has been built under a **star schema**, which contains a fact table and 4 dimension tables. The reason why it was decided to work under this type of scheme was that being a type of scheme that allows us to perform simple queries and quick aggregations, it would greatly facilitate the task of the Sparkify analytics team when consulting the data. Also, I have created staging tables before populating analytical tables (fact and dimension) in Redshift.

### Staging Tables
```
staging_events: Store the log information from json log data.
    - artist            VARCHAR,
    - auth              VARCHAR,
    - firstName         VARCHAR,
    - gender            VARCHAR,
    - itemInSession     INT,
    - lastName          VARCHAR,
    - length            NUMERIC,
    - level             VARCHAR,
    - location          VARCHAR,
    - method            VARCHAR,
    - page              VARCHAR   SORTKEY,
    - registration      BIGINT,
    - sessionId         BIGINT,
    - song              VARCHAR   DISTKEY,
    - status            INT,
    - ts                BIGINT,
    - userAgent         VARCHAR,
    - userId            INT
```

```
staging_songs: Store the song data before make transformations.
    - num_songs         INT,
    - artist_id         VARCHAR   DISTKEY,
    - artist_latitude   VARCHAR,
    - artist_longitude  VARCHAR,
    - artist_location   VARCHAR,
    - artist_name       VARCHAR,
    - song_id           VARCHAR,
    - title             VARCHAR,
    - duration          DECIMAL,
    - year              INT
```

### Fact Table
```
songplays: Store the records in log data associated with song plays i.e. records with page "NextSong"
    - songplay_id SERIAL PRIMARY KEY
    - start_time TIMESTAMP
    - user_id INT
    - level VARCHAR
    - song_id VARCHAR
    - artist_id VARCHAR
    - session_id INT
    - location VARCHAR
    - user_agent VARCHAR
```

### Dimension Tables
```
users: Store users in the app
    - user_id INT PRIMARY KEY
    - first_name VARCHAR
    - last_name VARCHAR
    - gender VARCHAR
    - level VARCHAR

songs: Store songs in music database
    - song_id VARCHAR PRIMARY KEY
    - title VARCHAR
    - artist_id VARCHAR
    - year INT
    - duration DECIMAL

artists: Store artists in music database
    - artist_id VARCHAR PRIMARY KEY
    - name VARCHAR
    - location VARCHAR
    - latitude DECIMAL
    - longitude DECIMAL

time: Store timestamps of records in songplays broken down into specific units
    - start_time TIMESTAMP PRIMARY KEY
    - hour INT
    - day INT
    - week INT
    - month INT
    - year INT
    - weekday INT
```

## Project Structure

```
|____assets			# Images used for README file
| |____...
|
|____dwh.cfg			# Configuration file
|____test_data.ipynb			# Testing ETL and validate results
|____sparkify.ipynb			# Create the data model and run the ETL
|
|____etl.py			# ETL builder
|____sql_queries.py			# Define query structure
|____create_tables.py			# Database/table creation script
|
|____README.md			# README file
```

In addition to the data files (data folder), the project workspace includes 8 files:

1. ```test_data.ipynb```: Check tables information against Redshift cluster.
2. ```create_tables.py```: Drop and create our tables. We run this file to reset our tables before each time we run our ETL scripts.
3. ```etl.py```: Read and process files from song_data and log_data, and loads them into our Redshift tables.
4. ```sql_queries.py```: Contains all our sql queries.
5. ```sparkify.ipynb```: Run the create_tables.py and etl.py files to create our sparkifydb database and tables, and run our ETL process respectively.
6. ```dwh.cfg```: Configuration file.
7. ```README.md```: Provide discussion on our project.

## ETL Pipeline

The first step in the ETL process is to connect to the ```sparkifydb``` database located in Redshift cluster, and create a cursor that allows us to execute our queries throughout the entire process.

As a next step we execute the ```create_tables.py``` file, which deletes all our tables if they exist, and then recreates them taking all the fields and contraints defined in their declaration. It should be noted that the dist and sort keys have been chosen considering the performance when making crosses between tables (joins).

Finally, the next and final step is to run the ```etl.py``` file, which copies the log and song data from their respective S3 buckets to the ```staging_events``` and ```staging_songs``` staging tables. Once the data is inserted in these staging tables, they are transformed and used as input for the fact and dimensonal tables.